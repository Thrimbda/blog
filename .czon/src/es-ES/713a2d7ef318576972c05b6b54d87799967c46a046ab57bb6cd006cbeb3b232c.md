---
"title": "Notas sobre el artículo de GFS"
"summary": "Este documento presenta notas sobre el artículo de GFS (Google File System), resumiendo las cuatro características principales de GFS como sistema de archivos distribuido: modelar las fallas de las máquinas, almacenar archivos grandes, operaciones de archivo centradas en la adición y un diseño que considera el escenario de aplicación. El artículo también detalla los supuestos de diseño de GFS, incluyendo que las fallas parciales son normales, los archivos grandes son comunes, las cargas de trabajo de lectura se dividen en grandes lecturas secuenciales y pequeñas lecturas aleatorias, las cargas de trabajo de escritura son principalmente grandes escrituras secuenciales, y la optimización para múltiples clientes que escriben en paralelo en el mismo archivo. Estas características y supuestos forman la base de la arquitectura de GFS, que es eficiente, confiable y adecuada para el procesamiento de datos a gran escala."
"tags":
  - "GFS"
  - "Sistemas Distribuidos"
  - "Sistema de Archivos"
  - "Notas de Artículo"
  - "Tecnología"
  - "Big Data"
  - "Almacenamiento"
  - "Google"
"date": "2020-10-29"
---

GFS es un sistema de archivos distribuido con las siguientes cuatro características:

1.  **Modela las fallas de las máquinas** - GFS se ejecuta en un clúster compuesto por una gran cantidad de hardware económico, por lo que el clúster experimentará fallas parciales por diversas razones.
2.  **Almacena archivos grandes** - (según los estándares de 2003) archivos grandes de varios GB son comunes, por lo que las operaciones de E/S y el tamaño de los bloques deben considerarse específicamente.
3.  **Las operaciones de archivo se centran principalmente en la adición** - Este es su principal objetivo de optimización.
4.  **El diseño del sistema de archivos considera el escenario de uso de la aplicación** - La flexibilidad mejora considerablemente, lo que es una ventaja de un sistema cerrado.

## Resumen del Diseño

### Supuestos

*   Las fallas parciales son normales - Se requiere monitoreo, verificación, tolerancia a fallos y capacidad de autoreparación.
*   Los archivos grandes son comunes - La optimización se centra principalmente en la gestión de archivos grandes.
*   Las cargas de trabajo de lectura son principalmente de dos tipos:
    *   **Grandes lecturas secuenciales (streaming reads)** - Aproximadamente 1 MB por lectura, lectura continua.
    *   **Pequeñas lecturas aleatorias (random reads)** - Unos pocos KB por lectura, aunque las aplicaciones tampoco toleran realmente las lecturas aleatorias.
*   Las cargas de trabajo de escritura son principalmente grandes escrituras secuenciales, similares a las operaciones de lectura, con pocas modificaciones, por lo que no se optimizan.
*   Se optimiza para que varios clientes escriban en paralelo en el mismo archivo - Logrado mediante operaciones atómicas bien diseñadas.