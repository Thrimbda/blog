---
"title": "Source Code Reading: Harbor-Operator Design Analysis"
"summary": "This article provides a detailed analysis of the source code design of Harbor-Operator, focusing on how it resolves dependencies between resources through a Dependency Graph, implements reconciliation logic for multiple CRDs using a single Controller, and improves code reusability by making code configurable via ResourceManager. The article also examines Harbor-Operator's practices on the Golang and Kubernetes platforms, showcasing it as an exemplary implementation of the 'the right thing' design philosophy. It demonstrates how to balance simplicity, consistency, and completeness, offering valuable design references for Operator development."
"tags":
  - "Harbor-Operator"
  - "Source Code Analysis"
  - "Kubernetes"
  - "Operator Pattern"
  - "Dependency Graph"
  - "Controller Design"
  - "Golang"
  - "Software Architecture"
"date": "2022-01-30"
---

<!--more-->

<a id="orgdafefdb"></a>

## Preparation Before Starting

<a id="org081213e"></a>

### Why Harbor-Operator?

<a id="org55a74c9"></a>

#### Worse is Better?

What is good software?

In the famous late 1980s article *The Rise of Worse is Better*, the author mentions that good software design should consider four qualities: simplicity, correctness, consistency, and completeness.

The author discusses two software design philosophies, tentatively named *the right thing* and *worse is better* (it's necessary to note that this naming is not pejorative). Both design philosophies revolve around the four qualities mentioned above, differing in their prioritization of these qualities.

---

<a id="orgbebeeee"></a>

##### The right thing

- **Simplicity** - The design must be simple and easy to understand. Interface simplicity is more important than implementation simplicity.
- **Correctness** - The design must be correct. There can be no compromise on this point.
- **Consistency** - Consistency is as important as correctness. Therefore, simplicity and completeness can be slightly compromised.
- **Completeness** - The design must consider various possible scenarios. Completeness should not be overly sacrificed for simplicity.

<a id="org948c614"></a>

##### Worse is better

- **Simplicity** - The design must be simple and easy to understand. Implementation simplicity is more important than interface simplicity. Simplicity is the most important quality.
- **Correctness** - The design must be correct, but simplicity is slightly more important than correctness.
- **Consistency** - The design should not be overly inconsistent. Consistency can be sacrificed for simplicity. Consistency can also be sacrificed for completeness, provided simplicity is maintained.
- **Completeness** - The design must consider various possible scenarios. Completeness can be sacrificed at any time to ensure design simplicity.

---

The author then provides many examples of both to argue why *worse-is-better* was sweeping the software industry at that time.

These two philosophies are not inherently superior or inferior to each other. Today, they coexist around us, and reality often oscillates between them. We hope to create excellent, aesthetically valuable designs but must also consider cost and human factors. Ultimately, we must deliver quality, working software to solve real-world problems, and the cost of writing and maintaining software must not exceed the value of the problem itself.

<a id="org514ce0b"></a>

#### Golang - the language

Golang is almost a paragon of the *worse is better* philosophy:

- It embodies Google's years of practical experience with C++.
- It is so simple, with no complex features, meaning there is essentially only one building block for solving a problem. This implies:
  - It can be easily mastered with little effort spent on the language itself.
  - The code is very readable.
- It compiles quickly, even sacrificing generics for compilation speed.
- Parallel tasks can be easily run via goroutines.
- It almost abandons the language feature of exception stacks, enforcing error checking and handling.
- Compile once, run anywhere.

<a id="org2432232"></a>

#### Kubernetes - the platform

Kubernetes is familiar to everyone, so I won't show off. In a nutshell: it provides a conceptually simple API design, supplemented by a reconciliation mechanism from control theory, enabling automated deployment, scaling, and operation of containerized software.

K8s allows developers to extend its capabilities by opening up this API specification and reconciliation mechanism. Developers can implement the Operator Pattern: encoding software configuration, deployment (Day-1), and operations, backup, failover (Day-2) knowledge into software that operates the software, automating these complex and error-prone operations to improve reliability and reduce costs.

---

The reason for choosing Harbor Operator is its excellent design. On an excellent platform and using simple language features, without excessive clever tricks, it implements a pragmatic and aesthetically valuable software system through several key designs that adhere to SOLID principles. It can be seen as a practitioner of the *the right thing* philosophy.

The author's code has almost no comments but is exceptionally readable, thanks to the entire system's...

<a id="org6815286"></a>

## Goals

Abstracting and simplifying problems inevitably comes with a sacrifice in flexibility. Harbor Operator completely sacrifices flexibility to achieve maximum mental relief, making Operator development more like declaring a software configuration.

Writing an operator manually using client-go, due to the lack of generics in Golang, is simply asking for trouble. The entire project would be filled with a massive amount of boilerplate code. I believe even if someone really wrote it manually with client-go, no one would start from scratch.

Thus, kubebuilder emerged. It abstracts operations like creating the Kubernetes Client, listening to requests from the Kubernetes API Server, and queuing requests into public libraries (controller runtime) and public tools (controller tools). It can generate scaffold code for developers, allowing them to focus on developing business logic for handling API object change requests.

Kubebuilder still preserves a bit of freedom for the diversity of business logic. Harbor Operator takes this a step further, pursuing extremes by completely sacrificing flexibility for conceptual consistency and simplicity. The business it faces is indeed very suitable for this approach.

Therefore, in this source code reading, our main goals are to learn how Harbor Operator:

- Performs Day-1 operations.
- Further reduces redundancy in operator code, using the same Controller code to implement Controllers for eleven different levels of CRDs.
- Utilizes DAG to resolve dependencies between resources. The author seems to have even patented this.

Additionally, we will not focus on:

- Day-2 operations in Harbor Operator; in fact, this part is not yet stable in the current version.
- The source code and functionality of Harbor itself.

<a id="org345a335"></a>

## Source Code Reading

<a id="org1198d5f"></a>

### Static Structure

<a id="orgb32de77"></a>

#### Directory Structure

Only directories are listed here.

```
    root
    ├── apis
    │   ├── goharbor.io
    │   │   └── v1alpha3
    │   └── meta
    │       └── v1alpha1
    ├── controllers
    │   ├── controller_string.go
    │   ├── controllers.go
    │   └── goharbor
    │       ├── chartmuseum
    │       ├── controller_test.go
    │       ├── core
    │       ├── exporter
    │       ├── harbor
    │       ├── harborcluster
    │       ├── internal
    │       ├── jobservice
    │       ├── notaryserver
    │       ├── notarysigner
    │       ├── portal
    │       ├── registry
    │       ├── registryctl
    │       └── trivy
    ├── pkg
    │   ├── builder
    │   ├── cluster
    │   │   ├── controllers
    │   │   │   ├── cache
    │   │   │   ├── common
    │   │   │   ├── database
    │   │   │   │   └── api
    │   │   │   ├── harbor
    │   │   │   └── storage
    │   │   ├── gos
    │   │   ├── k8s
    │   │   └── lcm
    │   ├── config
    │   │   ├── harbor
    │   │   └── template
    │   ├── controller
    │   │   ├── errors
    │   │   ├── internal
    │   │   └── graph
    │   │   └── mutation
    │   ├── event-filter
    │   ├── exit
    │   ├── factories
    │   │   ├── application
    │   │   ├── logger
    │   │   └── owner
    │   ├── graph
    │   ├── harbor
    │   ├── image
    │   ├── manager
    │   ├── resources
    │   │   ├── checksum
    │   │   └── statuscheck
    │   ├── scheme
    │   ├── setup
    │   ├── status
    │   ├── template
    │   ├── tracing
    │   ├── utils
    │   │   └── strings
    │   └── version
    ...
```

<a id="orgfc0f08e"></a>

#### Key Interfaces

![img](https://0xc1.space/images/2022/01/30/harbor-operator-class.svg)

<a id="org290c6c2"></a>

#### System Architecture

As of v1.0.1, Harbor Operator is primarily responsible for Day-1 operations of the Harbor system.
![img](https://0xc1.space/images/2022/01/30/harbor-operator-arch.png)

<a id="org41c8944"></a>

### Focus on the Big Picture: HarborCluster

First, let's filter out some less critical parts: the HarborCluster CRD and its Controller implementation.

Why is it special? First, observe its position in the system architecture: it's at the topmost layer. It manages the Harbor system itself and all its dependent stateful services. This needs to be explained from both the project's history and its special position in the system architecture.

From a system architecture perspective, the HarborCluster CRD definition is very similar to the Harbor CRD, with a lot of code redundancy, which doesn't look good. This is because, as the outermost (top-level) CRD in the system, it directly faces users. It must be able to provide users with all necessary configuration items for declaring a Harbor deployment. Additionally, since Harbor itself is a stateless service, a complete deployment also requires the HarborCluster CRD to manage all stateful services that Harbor depends on, including Postgres, Minio, and Redis.

The necessary information for the Harbor system itself is already defined in the Harbor CRD. Therefore, the redundant parts in the HarborCluster CRD are meant to pass this information completely and accurately to the Harbor CRD. Furthermore, the HarborCluster CRD also needs to manage the CRDs of those stateful services that are outside its own responsibility boundary, so it cannot fully use the Controller logic from Harbor and all its subcomponents.

From a historical perspective, Harbor Operator was originally a private project of OVH Cloud and was later donated to the goharbor community. Therefore, observing the git history, the reason for the significant inconsistency between the HarborCluster CRD definition and its Controller implementation compared to other Controllers in the system is that it was a later contribution from the community. The initial design of Harbor Operator did not consider the functions it would shoulder.

The HarborCluster Controller implementation itself is not much different from most Controllers implemented using Controller-runtime that we usually see, so it won't be studied in detail.

---

HarborCluster Controller

![img](https://0xc1.space/images/2022/01/30/harbor-cluster-controller.png)

---

Harbor Core Controller

![img](https://0xc1.space/images/2022/01/30/harbor-core-controller.png)

---

<a id="orgd9c3526"></a>

### Resolving Dependencies Between Resources: Dependency Graph

The dependency graph is a relatively independent module in the entire project, but it actually serves as the execution engine for all controllers in Harbor Operator. Essentially, it observes that various resources in Kubernetes have interdependencies. The deployment and reconciliation of some resources depend on the deployment and reconciliation of others. For example, a Deployment might depend on a ConfigMap. Ultimately, these dependencies form a dependency graph, which should actually be a DAG. Here, we need the interface defined as follows:

```go
    package graph

    type Resource interface{}

    type RunFunc func(context.Context, Resource) error

    type Manager interface {
            Run(ctx context.Context) error
            AddResource(ctx context.Context, resource Resource, blockers []Resource, run RunFunc) error
    }

    type resourceManager struct {
       resources map[Resource][]Resource
       functions map[Resource]RunFunc

       lock sync.Mutex
    }
```

- `Resource` defines an abstract resource. Since this module doesn't care what the resource represents, and to retain maximum flexibility under the constraints of language expressiveness, the top type `interface{}` is used.
- `RunFunc` is responsible for handling how to operate on a Resource. Here, `RunFunc` faces a type safety issue: `interface{}` means the compiler knows nothing about the type and can't proceed. But `RunFunc` must take this type, which could be anything, and do something with it. Therefore, we believe it must perform a type assertion. If every resource's `RunFunc` had to manually assert to a specific type, it would be very tedious and disgusting. Later, we'll see how Harbor Operator centralizes this dirty work.
- `Manager` has only two methods: adding a Resource and running the graph. We need to study them separately.
- The data structure `resourceManager` that implements `Manager` is also simple:
  - A map of `resource -> blockers`
  - A map of `resource -> runFunc`
  - A lock to handle concurrency for map operations. From this, we can see the author doesn't like data structures like `Sync.Map` that guarantee concurrency safety but lose type safety. This makes us even more curious about how the author handles so many (11 components) `RunFunc`s.

Since it's a Graph, there must be a Graph data structure and a factory method to build this Graph. The `resourceManager` data structure looks a bit rigid; it's uncertain if it's the actual Graph.

<a id="org75cbb07"></a>

#### AddResource

According to the signature, `AddResource` adds the resource itself, all its dependencies, and the corresponding `runFunc`. It's worth noting that resources that don't depend on any other resources (i.e., with out-degree 0) must be added first.

```go
    func (rm *resourceManager) AddResource(ctx context.Context, resource Resource, blockers []Resource, run RunFunc) error {
       if resource == nil {
          return nil
       }

       if run == nil {
          return errors.Errorf("unsupported RunFunc value %v", run)
       }

       span, _ := opentracing.StartSpanFromContext(ctx, "addResource", opentracing.Tags{
          "Resource": resource,
       })
       defer span.Finish()

       nonNilBlockers := []Resource{}

       for _, blocker := range blockers {
          if blocker == nil {
             continue
          }

          nonNilBlockers = append(nonNilBlockers, blocker)

          _, ok := rm.resources[blocker]
          if !ok {
             return errors.Errorf("unknown blocker %+v", blocker)
          }
       }

       rm.lock.Lock()
       defer rm.lock.Unlock()

       _, ok := rm.resources[resource]
       if ok {
          return errors.Errorf("resource %+v already added", resource)
       }

       rm.resources[resource] = nonNilBlockers
       rm.functions[resource] = run

       return nil
    }
```

<a id="org4cb9f3a"></a>

#### Run

Almost all the important logic in this package is in the `Run` method. Let's look at its first half:

```go
    func (rm *resourceManager) Run(ctx context.Context) error {
       span, ctx := opentracing.StartSpanFromContext(ctx, "walkGraph", opentracing.Tags{
          "Nodes.count": len(rm.resources),
       })
       defer span.Finish()

       g := errgroup.Group{}
       l := logger.Get(ctx)

       for _, no := range rm.getGraph(ctx) {
       ...
```

Here it is! `getGraph`. It looks like it's going to build the graph. Going in, we find that `resourceManager` is just a builder; the real graph is hidden inside, expressed using an adjacency list. At first glance, it even looks complex:

```go
    type node struct {
       resource Resource
       fn       RunFunc

       parent      chan error
       parentLock  *sync.Mutex
       parentCount int

       children     []chan<- error
       childrenLock []*sync.Mutex
    }

    func (no *node) Wait(ctx context.Context) error {...}
    func (no *node) Terminates(err error) (result error) {...}
    func (no *node) AddChild(child *node) {...}
```

Why are parent and children channels? Why so many locks? Let's look at the graph construction process:

```go
    func (rm *resourceManager) getGraph(ctx context.Context) []*node {
       span, _ := opentracing.StartSpanFromContext(ctx, "getGraph")
       defer span.Finish()

       rm.lock.Lock()
       defer rm.lock.Unlock()

       graph := make(map[Resource]*node, len(rm.resources))
       result := make([]*node, len(rm.resources))

       i := 0

       for resource, blockers := range rm.resources {
          blockerCount := len(blockers)

          node := &node{
             resource: resource,
             fn:       rm.functions[resource],

             parent:      make(chan error, blockerCount),
             parentLock:  &sync.Mutex{},
             parentCount: blockerCount,

             children:     []chan<- error{},
             childrenLock: []*sync.Mutex{},
          }
          graph[resource] = node
          result[i] = node

          i++

          blockers := blockers

          defer func() {
             for _, blocker := range blockers {
                graph[blocker].AddChild(node)
             }
          }()
       }

       return result
    }
```

So the dependency relationship is inverted. Now, the depended-upon resource points to the dependent. For each resource, a node is constructed. Simultaneously, for each dependency of the resource, the resource's parent channel is added