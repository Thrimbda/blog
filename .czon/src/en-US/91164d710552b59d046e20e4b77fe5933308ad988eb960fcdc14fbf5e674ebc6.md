---
"title": "Manually Setting Up Prometheus on a Single-Node K8s Cluster"
"summary": "This article is a detailed guide on manually setting up the Prometheus monitoring system on a single-node Kubernetes cluster. It begins with a proof-of-concept on a bare-metal environment, running Prometheus and Node Exporter to understand basic configurations. The focus then shifts to deploying within a K8s cluster, detailing the required K8s resources (such as Namespace, DaemonSet, ConfigMap, ServiceAccount, ClusterRole, etc.) and explaining how to configure Prometheus's service discovery (specifically kubernetes_sd_config) and relabeling (relabel_config) to monitor multiple targets, including Prometheus itself, Node Exporter, Kubelet, cAdvisor, and the API Server. The article emphasizes manual configuration (rather than using Helm or Operator) to gain a deep understanding of how Prometheus works, providing configuration examples and permission settings. Finally, it summarizes the deployment steps and leaves a thought-provoking question on how to monitor a K8s cluster with a bare-metal Prometheus."
"tags":
  - "Observability"
  - "Prometheus"
  - "Kubernetes"
  - "Monitoring"
  - "Service Discovery"
  - "Node Exporter"
  - "cAdvisor"
  - "Kubelet"
"date": "2020-11-05"
---

> The target audience of this article is those who are just starting to explore monitoring systems and have limited knowledge of Prometheus (much like the author when writing this article).
>
>
>
> Environment used for setting up Prometheus in this article:
>
> - K8s version: 1.19.3
> - Prometheus version: 2.22.0
> - Operating System: Archlinux as of 2020.11
> - Hosts configured, domain name for Devbox is devbox
>
> ⚠️ Note: The command-line parameters listed in this article need to be adjusted according to the current environment (e.g., Prometheus binary version, etc.).
>
>
>
> Here are some recommended prerequisites:
>
> 1. [Observability: Concepts and Best Practices](https://github.com/lichuan0620/k8s-sre-learning-notes/blob/master/observability/OBSV-101.md) introduces various basic concepts of observability.
> 2. [Getting to Know Prometheus](https://github.com/lichuan0620/k8s-sre-learning-notes/blob/master/prometheus/PROM-101.md) introduces the Prometheus project.
> 3. [Introduction on the Prometheus Official Website](https://prometheus.io/docs/introduction/overview/)

## Objective

Since we are manually setting up Prometheus on K8s, we have two conventions here:

1. Deliberately avoid using quick deployment methods like Helm-Chart or Prometheus Operator. Here are some for reference:
   1. Prometheus community-maintained [Helm chart](https://github.com/prometheus-community/helm-charts)
   2. [Prometheus Operator](https://github.com/prometheus-operator/prometheus-operator)
   3. [Kube-Prometheus](https://github.com/prometheus-operator/kube-prometheus)
2. Set up Prometheus on K8s, meaning K8s manages the Prometheus service. Unlike the Prometheus Operator mentioned above, we will write the relevant YAML configuration files ourselves.
3. List the following monitoring targets:
   1. Prometheus
   2. Node exporter
   3. Kubelet
   4. cAdvisor
   5. API Server

Let's get started!

<!--more-->

## Proof-of-Concept: Running Prometheus on Bare Metal

First, the intuition is to do a proof-of-concept on bare metal. Start by running it and experimenting with further configurations. Ultimately, once we understand Prometheus's various configuration items, redeploying it on K8s should be straightforward.

> I tried to be lazy and searched for tutorial blogs, but found them unclear and mostly outdated. As a result, I wasted half a day and had to read the official documentation carefully.

### Installing Prometheus

According to the [documentation](https://prometheus.io/docs/prometheus/2.22/getting_started/), download the corresponding pre-compiled binary package directly [here](https://prometheus.io/download/):

```bash
curl -LO "https://github.com/prometheus/prometheus/releases/download/v2.22.0/prometheus-2.22.0.linux-amd64.tar.gz"
tar -zxvf prometheus-2.22.0.linux-amd64.tar.gz
cd prometheus-2.22.0.linux-amd64
./prometheus --version
# expected output should be like this:
# prometheus, version 2.22.0 (branch: HEAD, revision: 0a7fdd3b76960808c3a91d92267c3d815c1bc354)
#  build user:    root@6321101b2c50
#  build date:    20201015-12:29:59
#  go version:    go1.15.3
#  platform:     linux/amd64
```

Check the directory and find a configuration file prometheus.yml included:

```yaml
# my global config
global:
 scrape_interval:   15s # Set the scrape interval to every 15 seconds. Default is every 1 minute.
 evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute.
 # scrape_timeout is set to the global default (10s).
# Alertmanager configuration
alerting:
 alertmanagers:
 - static_configs:
  - targets:
   # - alertmanager:9093
# Load rules once and periodically evaluate them according to the global 'evaluation_interval'.
rule_files:
 # - "first_rules.yml"
 # - "second_rules.yml"
# A scrape configuration containing exactly one endpoint to scrape:
# Here it's Prometheus itself.
scrape_configs:
 # The job name is added as a label `job=<job_name>` to any timeseries scraped from this config.
 - job_name: 'prometheus'
  # metrics_path defaults to '/metrics'
  # scheme defaults to 'http'.
  static_configs:
  - targets: ['localhost:9090']
```

Now, we run the downloaded Prometheus to monitor itself, achieving a small feedback loop of satisfaction:

```bash
./prometheus --config.file=prometheus.yml
```

You can see that Prometheus has started. Access http://devbox:9090 to see its user interface. At this point, click around randomly to get a general sense of Prometheus's features, giving us an understanding of how Prometheus behaves when running normally.

### Running Node Exporter

Now, let's run a Node Exporter on bare metal to observe various metrics of the local machine.

```bash
curl -LO "https://github.com/prometheus/node_exporter/releases/download/v1.0.1/node_exporter-1.0.1.linux-amd64.tar.gz"
tar -zxvf node_exporter-1.0.1.linux-amd64.tar.gz
cd node_exporter-1.0.1.linux-amd64
./node_exporter
```

Next, modify the configuration to let Prometheus collect metrics from it.

```yaml
# my global config
global:
 scrape_interval:   15s # Set the scrape interval to every 15 seconds. Default is every 1 minute.
 evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute.
 # scrape_timeout is set to the global default (10s).
# A scrape configuration containing exactly one endpoint to scrape:
# Here it's Prometheus itself.
scrape_configs:
 # The job name is added as a label `job=<job_name>` to any timeseries scraped from this config.
 - job_name: 'prometheus'
  # metrics_path defaults to '/metrics'
  # scheme defaults to 'http'.
  static_configs:
  - targets: ['localhost:9090']
 - job_name: 'node-exporter'
  static_configs:
  - targets: ['localhost:9100']
```

Open Prometheus's web UI and observe that a new target named `node-exporter` has been added. Check the workload (running a program that can occupy all cores by continuously calculating the Fibonacci sequence [here](https://github.com/Thrimbda/fiber)):

![img](https://0xc1.space/images/2020/11/05/node-load.png)

At this point, the proof-of-concept phase is successfully completed.

> Note: As a proof-of-concept, it is not recommended to directly use a bare-metal deployed Prometheus to monitor a K8s cluster because accessing K8s components from outside the cluster requires configuring certificates and having the appropriate access permissions [ClusterRole](https://kubernetes.io/zh/docs/reference/access-authn-authz/rbac/) (omitting the various pitfalls the author encountered when trying to monitor a K8s cluster and its components with a bare-metal deployed Prometheus).

## Monitoring the K8s Cluster with Prometheus

Next, we want to monitor our K8s cluster through Prometheus.

### Prometheus Configuration Items

From the introduction to Prometheus, we know that Prometheus is primarily Pull-based for data acquisition, so service discovery is needed—letting Prometheus know where to pull data from so users can view it.

So, the first problem to solve is: **Service discovery for the K8s cluster**—the secret must be hidden in the configuration.

The [documentation](https://prometheus.io/docs/prometheus/2.22/configuration/configuration/) provides a detailed description of Prometheus configuration.

Here is a brief description of the following configuration items (they are not necessarily orthogonal to each other):

- [`<global>`](https://prometheus.io/docs/prometheus/2.22/configuration/configuration/#configuration-file): The configuration here applies to any other configuration item and serves as the default value for items in other configurations.
- [`<scrape_config>`](https://prometheus.io/docs/prometheus/2.22/configuration/configuration/#scrape_config): Defines a monitoring task, describing where and how Prometheus should monitor this target, among other information.
- [`<tls_config>`](https://prometheus.io/docs/prometheus/2.22/configuration/configuration/#tls_config): Describes TLS configuration.
- [`<*_sd_config>`](https://prometheus.io/docs/prometheus/2.22/configuration/configuration/#kubernetes_sd_config): Prometheus provides configuration for service discovery of a series of predefined monitoring targets through this series of configuration items (sd stands for service discovery).
- [`<static_config>`](https://prometheus.io/docs/prometheus/2.22/configuration/configuration/#static_config): For monitoring targets not predefined by Prometheus (such as any service manually deployed on bare metal), this configuration item can be used for service discovery. We used this configuration item in the proof-of-concept above.
- [`<relabel_config>`](https://prometheus.io/docs/prometheus/2.22/configuration/configuration/#relabel_config): Before starting to pull metrics from monitoring targets, this configuration item can be used to change some labels. Prometheus provides some predefined label rules. Relabeling can be done in multiple steps. After relabeling, labels prefixed with __ are deleted.

It seems that the core configuration item in Prometheus is `<scrape_config>`, each defining a monitoring task, similar to the concept of a namespace, mainly providing aggregation of monitoring targets. Within it, we define `<*_sd_config>` or `<static_config>` to tell Prometheus specifically from which endpoints to pull data and how to filter these endpoints.

Next, let's deepen our understanding of these configuration items through practice!

### Deploying Prometheus

The core work of deployment lies in thinking clearly about what resources are needed to deploy Prometheus in the cluster. The author directly provides the answer here:

1. Dedicated Namespace
2. A DaemonSet to manage node-exporter
3. Node-exporter Service
4. Manage Prometheus configuration with ConfigMap
5. Dedicated ServiceAccount for Prometheus
6. ClusterRole with sufficient permissions
7. ClusterRoleBinding that binds the ServiceAccount and ClusterRole together
8. Prometheus Deployment
9. Prometheus Service

On a K8s cluster with RBAC applied, we need to define a role with sufficient permissions for Prometheus to read the cluster status and various metrics, hence items 5-7.

Here is a [collection of resource declarations](https://github.com/Thrimbda/prometheus-set-up) accumulated by the author during the setup process. In addition to the above resources, it also includes kube-state-metrics. Follow the steps in order to get a deployed Prometheus.

#### Node-exporter

For Node-exporter, since it monitors the machine itself, the requirement is one per Node. Since we also want to enjoy K8s lifecycle management, DaemonSet is the best choice.

Since it runs in a container, without configuration, it cannot collect real Node metrics. Therefore, it is necessary to mount special locations from the host into the container so that Node-exporter can collect metrics.

```yaml
args:
- '--path.procfs=/host/proc'
- '--path.sysfs=/host/sys'
- '--path.rootfs=/host/root'
volumes:
- hostPath:
  path: /proc
 name: proc
- hostPath:
  path: /sys
 name: sys
- hostPath:
  path: /
 name: roo
```

Then expose an endpoint that Prometheus can access long-term through a Service.

#### Prometheus

Prometheus is deployed using a Deployment. Before deploying Prometheus, it needs to be configured with sufficient permissions to access necessary endpoints to collect metrics. In a K8s cluster with RBAC configured, this is achieved through ClusterRole/ServiceAccount/ClusterRoleBinding. After configuration, Prometheus uses the ServiceAccount for authentication to access the required endpoints.

```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
 name: prometheus
 labels:
  app.kubernetes.io/name: prometheus
rules:
 - apiGroups: [""]
  resources:
  - nodes
  - nodes/metrics
  - services
  - endpoints
  - pods
  verbs: ["get", "list", "watch"]
 - nonResourceURLs:
  - "/metrics"
  - "/metrics/cadviror"
  verbs: ["get"]
---
apiVersion: v1
kind: ServiceAccount
metadata:
 name: default
 namespace: monitoring-system
 labels:
  app.kubernetes.io/name: prometheus
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
 name: promtheus
 labels:
  app.kubernetes.io/name: prometheus
roleRef:
 apiGroup: rbac.authorization.k8s.io
 kind: ClusterRole
 name: prometheus
subjects:
- kind: ServiceAccount
 name: default
 namespace: monitoring-system
```

So far, we have all the prerequisites to achieve our monitoring goals. So how do we drive the powerful engine of Prometheus to make full use of the environment we have set up to achieve monitoring?

Combining the introduction to Prometheus configuration earlier, the four monitoring targets are defined with four `<scrape_config>`:

For node-exporter:

```yaml
- job_name: 'node-exporter'
 kubernetes_sd_configs:
 - role: endpoints
 relabel_configs:
 - source_labels: [__meta_kubernetes_service_name]
  regex: node-exporter
  action: keep
 - source_labels: [__meta_kubernetes_endpoint_node_name]
  target_label: node
 - source_labels: [__meta_kubernetes_pod_host_ip]
  target_label: host_ip
```

Since it is inside the cluster, no additional authentication is required, and HTTPS access is not needed.

Here, we further explain `<relabel_configs>` using the node-exporter example:

A label is an attribute of a certain endpoint, and different endpoints may have different values under the same label. What `<relabel_config>` does is perform some modification and filtering operations on these labels, allowing us to filter/modify the desired endpoints.

![img](https://0xc1.space/images/2020/11/05/node-exporter-target.png)

As you can see, in the above config, there are three relabel actions. The first one means: from all values of the K8s service discovery **predefined** label `__meta_kubernetes_service_name`, filter according to the given regular expression "node-exporter", and based on the `action`, keep the matching target endpoints and discard the remaining values with the same label. The latter two relabel actions are to retain the semantic labels of node and host_ip by renaming them. (Remember, labels starting with double underscores are eventually deleted.)

For Prometheus itself:

```yaml
- job_name: 'prometheus'
 kubernetes_sd_configs:
 - role: endpoints
 relabel_configs:
 - source_labels: [__meta_kubernetes_service_name]
  regex: prometheus
  action: kee
```

Use the same trick to filter out endpoints.

For kubelet and cAdvisor, the situation becomes slightly more complicated:

```yaml
- job_name: 'kubelet'
 kubernetes_sd_configs:
 - role: node
 tls_config:
  # ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
  # cert_file: /etc/secret/cert
  # key_file: /etc/secret/key
  insecure_skip_verify: true
 bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
 scheme: https
- job_name: 'cadvisor'
 kubernetes_sd_configs:
 - role: node
 metrics_path: /metrics/cadvisor
 tls_config:
  # ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
  # cert_file: /etc/secret/cert
  # key_file: /etc/secret/key
  insecure_skip_verify: true
 bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
 scheme: https
```

Note that the role becomes node, so Prometheus will collect metrics from `<node_ip>:10250/metrics` by default. Here, there is an additional `bearer_token_file` configuration item. Since kubelet does not allow anonymous access to its metrics by default, this is where the ServiceAccount configured earlier is used. For convenience, we use `insecure_skip_verify: true` to skip TLS authentication.

For the API Server, it becomes a bit more complicated again:

```yaml
scrape_configs:
- job_name: 'kubernetes-apiservers'
 kubernetes_sd_configs:
 - role: endpoints
 scheme: https
 tls_config:
  ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
 bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
 relabel_configs:
 - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]
  action: keep
  regex: default;kubernetes;https
```

Here, we use `<relabel_config>` to filter the API Server's own endpoints. While providing token authentication, we also need to provide a CA file for identity verification, so we can access the API Server.

Thus, we have completed the deployment of Prometheus and the monitoring configuration for the target endpoints.

Interested readers can further modify the config to observe Prometheus's behavior under different configurations to deepen understanding. Here is a small assignment: How can we monitor a K8s cluster with a bare-metal deployed Prometheus?

## References

1. [Prometheus Configuration](https://prometheus.io/docs/prometheus/2.22/configuration/configuration/#configuration)
2. [Kube-prometheus manifests](https://github.com/prometheus-operator/kube-prometheus/tree/8b0eebdd08d8926649d27d2bc23acf31144c2f6b/manifests)
3. [TSDB v3 design](https://fabxc.org/tsdb/)
4. [Observability: Concepts and Best Practices](https://github.com/lichuan0620/k8s-sre-learning-notes/blob/master/observability/OBSV-101.md)
5. [Getting to Know Prometheus](https://github.com/lichuan0620/k8s-sre-learning-notes/blob/master/observability/OBSV-101.md)
6. [RBAC on K8s](https://kubernetes.io/zh/docs/reference/access-authn-authz/rbac/)