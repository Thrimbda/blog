---
"title": "GFS Paper Notes"
"summary": "This article provides notes on the GFS (Google File System) paper, summarizing the four core characteristics of GFS as a distributed file system: modeling machine failures, storing large files, primarily appending to files, and designing with application scenarios in mind. The article also elaborates on GFS's design assumptions, including that partial failures are the norm, large files are the norm, read workloads consist of large streaming reads and small random reads, write workloads are primarily large sequential writes, and optimizations for multiple clients writing to the same file concurrently. These characteristics and assumptions together form the foundation of GFS's efficient, reliable, and scalable architecture for large-scale data processing."
"tags":
  - "GFS"
  - "Distributed Systems"
  - "File System"
  - "Paper Notes"
  - "Technology"
  - "Big Data"
  - "Storage"
  - "Google"
"date": "October 29, 2020"
---

GFS is a distributed file system with the following four characteristics:

1. **Modeling Machine Failures** - GFS runs on a cluster composed of a large number of inexpensive hardware components, so the cluster is prone to partial failures due to various reasons.
2. **Storing Large Files** - (By 2003 standards) large files of several GB are the norm, so I/O operations and block sizes need to be specifically considered.
3. **File Operations Primarily Append** - This is its main optimization target.
4. **File System Design Considers Application Scenarios** - Greatly improves flexibility, which is a benefit of a closed system.

## Design Overview

### Assumptions

- Partial failures are the norm - Requires monitoring, checking, error tolerance, and self-healing capabilities.
- Large files are the norm - Optimizations are primarily focused on managing large files.
- Read workloads mainly consist of the following two types:
  - Large streaming reads - Around 1MB per read, continuous reading.
  - Small random reads - A few KB per read, although applications do not truly tolerate random reads.
- Write workloads are primarily large sequential writes, similar to read operations, with few modifications, so no optimizations are made for them.
- Optimized for multiple clients writing to the same file concurrently - Achieved through well-defined atomic operations.